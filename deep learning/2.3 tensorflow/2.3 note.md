### 第三周 超参数调试、Batch norm和程序框架

#### **超参数调试处理** 

- 在机器学习领域，超参数比较少的情况下，我们之前利用设置网格点的方式来调试超参数；根据固定的组合分析那个参数影响较大，但是，固定的网格效率很低，如图，如果说有三个参数，相当于训练125个模型，分别只尝试了5个$\alpha,\beta,\gamma$ 
- 但在深度学习领域，超参数较多的情况下，不是设置规则的网格点，而是随机选择点进行调试。这样做是因为在我们处理问题的时候，是无法知道哪个超参数是更重要的，所以随机的方式去测试超参数点的性能，更为合理，这样可以探究更超参数的潜在价值，通过聚焦后在进行搜索，这样效率更高。

![10](F:\program\neural_network\note_pictures\10.png)

![11](F:\program\neural_network\note_pictures\11.jpg)

#### 为超参数选择合适的范围 

- 均匀随机scale

  在超参数选择的时候，一些超参数是在一个范围内进行均匀随机取值，如隐藏层神经元结点的个数、隐藏层的层数等。但是有些是不能进行均匀随机选取的，比如学习率$\alpha$ ，在0.001到1之间，当学习率较大是，网络中的参数对学习率变化的敏感程度比学习率小的时候对其敏感度要小，也就是说，如果在$0.001\sim1$ 的范围内进行进行均匀随机取值，则有90%的概率 选择范围在$0.1\sim1$ 之间，而只有10%的概率才能选择到$0.001\sim 0.1$ 之间，显然是不合理的。（因为0.001与0.002带来的差别明显要大于0.9001和0.9002所带来的差别）

  所以，在选择时，需要对其进行分区平均，如$0.001\sim0.01,0.01\sim0.1$ 

- 使用指数加权平均

  如果参数在$10^a\sim10^b$ 之间需要使用指数平均进行搜索，如学习率$\alpha$ ，则 $r\in[a,b] $   ,$\alpha=10^r$ 

  ```python
  r = -4 * np.random.rand()     # r in [-4,0]
  learning_rate = 10 ** r     ` # 10^{r}
  ```

#### 超参数调试实践

- 在超参数调试的实际操作中，我们需要根据我们现有的计算资源来决定以什么样的方式去调试超参数，进而对模型进行改进。下面是不同情况下的两种方式：
- 熊猫： 专注训练一个模型，实时关注，根据需要调整超参数，不断优化
  - 适用人群：数据量大，计算能力有限
  - 至少learning_rate $\alpha$, momentum $\beta$ , Ng提及可以自由调试
- 鱼卵：同时训练多个模型（不同超参数提前设定好），最后对比，找到最优超参数值
  - 适用人群： 计算能力特别强大
  - 多个模型的区别：
    - 超参数的区别（相同模型，不同超参数）
    - 模型本质区别（不同模型，不同超参数）

![12](F:\program\neural_network\note_pictures\12.jpg)

#### Batch Normalization

- 回想一下，在logistic regression中，为了加速模型的训练，将输出数据进行归一化处理（将数据减去平均，除以方差）。对于深层的神经网路，我们通过对隐藏层的输出$a^{[l]}$ 或激活函数前的$z^{[l]}$ 进行归一化处理，可以对神经网络进行加速。

  一般常用的方法是对激活函数前的$z^{[l]}$ 进行归一化处理

  ![11](F:\program\neural_network\note_pictures\11.png)

- 对于每一个batch，假设神经网络的某一个中间值为 ：$z^{(1)},z^{(2)},\dots,z^{(m)}$ 

  $\mu = \dfrac{1}{m}\sum\limits_{i}z^{(i)}$ 

  $\sigma^{2}=\dfrac{1}{m}\sum\limits_{i}(z^{(i)}-\mu)^{2}$ 

  $z^{(i)}_{\rm norm} = \dfrac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}}$ 

  这里增加$\varepsilon$ 是为了保证数值的稳定。

  为了不让所有的隐藏层都是均值为0，方差为1的分布，增加两个参数$\gamma,\beta$ ，然后再进行计算

  $\widetilde z^{(i)} = \gamma z^{(i)}_{\rm norm}+\beta$ 

  这里 $\gamma$ 和 $\beta$ 是可以更新学习的参数，如神经网络的权重$w$  一样，两个参数的值来确定 $\widetilde z^{(i)}$  所属的分布。

