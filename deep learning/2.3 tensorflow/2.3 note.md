### 第三周 超参数调试、Batch norm和程序框架

#### **超参数调试处理** 

- 在机器学习领域，超参数比较少的情况下，我们之前利用设置网格点的方式来调试超参数；根据固定的组合分析那个参数影响较大，但是，固定的网格效率很低，如图，如果说有三个参数，相当于训练125个模型，分别只尝试了5个$\alpha,\beta,\gamma$ 
- 但在深度学习领域，超参数较多的情况下，不是设置规则的网格点，而是随机选择点进行调试。这样做是因为在我们处理问题的时候，是无法知道哪个超参数是更重要的，所以随机的方式去测试超参数点的性能，更为合理，这样可以探究更超参数的潜在价值，通过聚焦后在进行搜索，这样效率更高。

![10](F:\program\neural_network\note_pictures\10.png)

![11](F:\program\neural_network\note_pictures\11.jpg)

#### 为超参数选择合适的范围 

- 均匀随机scale

  在超参数选择的时候，一些超参数是在一个范围内进行均匀随机取值，如隐藏层神经元结点的个数、隐藏层的层数等。但是有些是不能进行均匀随机选取的，比如学习率$\alpha$ ，在0.001到1之间，当学习率较大是，网络中的参数对学习率变化的敏感程度比学习率小的时候对其敏感度要小，也就是说，如果在$0.001\sim1$ 的范围内进行进行均匀随机取值，则有90%的概率 选择范围在$0.1\sim1$ 之间，而只有10%的概率才能选择到$0.001\sim 0.1$ 之间，显然是不合理的。（因为0.001与0.002带来的差别明显要大于0.9001和0.9002所带来的差别）

  所以，在选择时，需要对其进行分区平均，如$0.001\sim0.01,0.01\sim0.1$ 

- 使用指数加权平均

  如果参数在$10^a\sim10^b$ 之间需要使用指数平均进行搜索，如学习率$\alpha$ ，则 $r\in[a,b] $   ,$\alpha=10^r$ 

  ```python
  r = -4 * np.random.rand()     # r in [-4,0]
  learning_rate = 10 ** r     ` # 10^{r}
  ```

#### 超参数调试实践

- 在超参数调试的实际操作中，我们需要根据我们现有的计算资源来决定以什么样的方式去调试超参数，进而对模型进行改进。下面是不同情况下的两种方式：
- 熊猫： 专注训练一个模型，实时关注，根据需要调整超参数，不断优化
  - 适用人群：数据量大，计算能力有限
  - 至少learning_rate $\alpha$, momentum $\beta$ , Ng提及可以自由调试
- 鱼卵：同时训练多个模型（不同超参数提前设定好），最后对比，找到最优超参数值
  - 适用人群： 计算能力特别强大
  - 多个模型的区别：
    - 超参数的区别（相同模型，不同超参数）
    - 模型本质区别（不同模型，不同超参数）

![12](F:\program\neural_network\note_pictures\12.jpg)

#### Batch Normalization

- 回想一下，在logistic regression中，为了加速模型的训练，将输出数据进行归一化处理（将数据减去平均，除以方差）。对于深层的神经网路，我们通过对隐藏层的输出$a^{[l]}$ 或激活函数前的$z^{[l]}$ 进行归一化处理，可以对神经网络进行加速。

  一般常用的方法是对激活函数前的$z^{[l]}$ 进行归一化处理

  ![11](F:\program\neural_network\note_pictures\11.png)

- 对于每一个batch，假设神经网络的某一个中间值为 ：$z^{(1)},z^{(2)},\dots,z^{(m)}$ 

  $\mu = \dfrac{1}{m}\sum\limits_{i}z^{(i)}$ 

  $\sigma^{2}=\dfrac{1}{m}\sum\limits_{i}(z^{(i)}-\mu)^{2}$ 

  $z^{(i)}_{\rm norm} = \dfrac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}}$ 

  这里增加$\varepsilon$ 是为了保证数值的稳定。

  为了不让所有的隐藏层都是均值为0，方差为1的分布，增加两个参数$\gamma,\beta$ ，然后再进行计算

  $\widetilde z^{(i)} = \gamma z^{(i)}_{\rm norm}+\beta$ 

  这里 $\gamma$ 和 $\beta$ 是可以更新学习的参数，如神经网络的权重$w$  一样，两个参数的值来确定 $\widetilde z^{(i)}$  所属的分布。


#### 神经网络中的Batch Norm

- 在深度神经网络中，计算流程如下

  ![12](F:\program\neural_network\note_pictures\12.png)

  在该网络中，在计算激活函数前使用了正则化来加速训练

- 梯度下降的实现

  for t = 1 ... num （这里num 为Mini Batch 的数量）：

  - - 在每一个!$X^{t}$ 上进行前向传播（forward prop）的计算：

  - - - 在每个隐藏层都用 Batch Norm 将 $z^{[l]}$  替换为 $\widetilde z^{[l]}$ 

    - 使用反向传播（Back prop）计算各个参数的梯度： $dw^{[l]}、d\gamma^{[l]}、d\beta^{[l]}$ 

    - 更新参数：

    - $w^{[l]}:=w^{[l]}-\alpha dw^{[l]}$ 

    - $\gamma^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]}$ 

    - $\beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]}$ 

  - 同样与Mini-batch 梯度下降法相同，Batch Norm同样适用于momentum、RMSprop、Adam的梯度下降法来进行参数更新

- 在使用batch norm时，可以将$b^{l}$ 省略掉，因为 ![z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}](http://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D%3Dw%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D) ，而Batch Norm 要做的就是将 $z^{[l]}$  归一化，结果成为均值为0，标准差为1的分布，再由 $\beta$  和 $\gamma$  进行重新的分布缩放，那就是意味着，无论 $b^{[l]}$  值为多少，在这个过程中都会被减去，不会再起作用

![13](F:\program\neural_network\note_pictures\13.png)

####  Batch Norm 起作用的原因

**First Reason**

首先Batch Norm 可以加速神经网络训练的原因和输入层的输入特征进行归一化，从而改变Cost function的形状，使得每一次梯度下降都可以更快的接近函数的最小值点，从而加速模型训练过程的原理是有相同的道理。

只是Batch Norm 不是单纯的将输入的特征进行归一化，而是将各个隐藏层的激活函数的激活值进行的归一化，并调整到另外的分布。

**Second Reason**

Batch Norm 可以加速神经网络训练的另外一个原因是它可以使权重比网络更滞后或者更深层。

下面是一个判别是否是猫的分类问题，假设第一训练样本的集合中的猫均是黑猫，而第二个训练样本集合中的猫是各种颜色的猫。如果我们将第二个训练样本直接输入到用第一个训练样本集合训练出的模型进行分类判别，那么我们在很大程度上是无法保证能够得到很好的判别结果。

这是因为第一个训练集合中均是黑猫，而第二个训练集合中各色猫均有，虽然都是猫，但是很大程度上样本的分布情况是不同的，所以我们无法保证模型可以仅仅通过黑色猫的样本就可以完美的找到完整的决策边界。第二个样本集合相当于第一个样本的分布的改变，称为：Covariate shift。如下图所示：

![13](F:\program\neural_network\note_pictures\13.jpg)

那么存在Covariate shift的问题如何应用在神经网络中？就是利用**Batch Norm**来实现。如下面的网络结构：

![14](F:\program\neural_network\note_pictures\14.jpg)

网络的目的是通过不断的训练，最后输出一个更加接近于真实值的 $\hat y$  。现在以第2个隐藏层为输入来看：

![15](F:\program\neural_network\note_pictures\15.jpg)

对于后面的神经网络，是以第二层隐层的输出值 $a^{[2]}$  作为输入特征的，通过前向传播得到最终的 $\hat y$  ，但是因为我们的网络还有前面两层，由于训练过程，参数 $w^{[1]}，w^{[2]}$  是不断变化的，那么也就是说对于后面的网络， $a^{[2]}$  的值也是处于不断变化之中，所以就有了Covariate shift的问题。

那么如果对 $z^{[2]}$ 使用了Batch Norm，那么即使其值不断的变化，但是其均值和方差却会保持。那么Batch Norm的作用便是其限制了前层的参数更新导致对后面网络数值分布程度的影响，使得输入后层的数值变得更加稳定。另一个角度就是可以看作，Batch Norm 削弱了前层参数与后层参数之间的联系，使得网络的每层都可以自己进行学习，相对其他层有一定的独立性，这会有助于加速整个网络的学习。

#### 在测试数据上使用 Batch Norm

在训练中，对所有层的均值方差的不同样本下的值，做指数加权平均，最后的均值和方差，用在test的模型里来计算batch norm.

![14](F:\program\neural_network\note_pictures\14.png)

#### Softmax 回归

在多分类问题中，有一种 logistic regression的一般形式，叫做Softmax regression。Softmax回归可以将多分类任务的输出转换为各个类别可能的概率，从而将最大的概率值所对应的类别作为输入样本的输出类别

![16](F:\program\neural_network\note_pictures\16.jpg)

![15](F:\program\neural_network\note_pictures\15.png)

#### 训练一个softmax分类模型

- 为什么叫softmax，如图

![16](F:\program\neural_network\note_pictures\16.png)

- 通常我们判定模型的输出类别，是将输出的最大值对应的类别判定为该模型的类别，也就是说最大值为的位置1，其余位置为0，这也就是所谓的“hardmax”。而Sotfmax将模型判定的类别由原来的最大数字5，变为了一个最大的概率0.842，这相对于 “hardmax”而言，输出更加“soft”而没有那么“hard”。Sotfmax回归 将 logistic回归 从二分类问题推广到了多分类问题上。

- **Softmax 的 Loss function**

  在使用Sotfmax层时，对应的目标值 y  以及训练结束前某次的输出的概率值 $\hat y$ 分别为：

  $y=\left[ \begin{array}{l} 0\\1\\0\\0 \end{array} \right] , \ \hat y=\left[ \begin{array}{l} 0.3\\0.2\\0.1\\0.4 \end{array} \right]$ 

  Sotfmax使用的 Loss function 为：

  $L(\hat y,y)=-\sum\limits_{j=1}^{4}y_{j}\log \hat y_{j}$ 

  在训练过程中，我们的目标是最小化Loss function，由目标值我们可以知道， $y_{1}=y_{3}=y_{4}=0，y_{2}=1$  ，所以代入 $L(\hat y,y)$  中，有：

  $L(\hat y,y)=-\sum\limits_{j=1}^{4}y_{j}\log \hat y_{j}=-y_{2}\log \hat y_{2}=-\log \hat y_{2}$ 

  所以为了最小化Loss function，我们的目标就变成了使得 $\hat y_{2}$  的概率尽可能的大。

  也就是说，这里的损失函数的作用就是找到你训练集中的真实的类别，然后使得该类别相应的概率尽可能地高，这其实是最大似然估计的一种形式。

  对应的Cost function如下：

  $J(w^{[1]},b^{[1]},\ldots)=\dfrac{1}{m}\sum\limits_{i=1}^{m}L(\hat y^{(i)},y^{(i)})$ 

  **Softmax 的梯度下降**

  在Softmax层的梯度计算公式为：

  $\dfrac{\partial J}{\partial z^{[L]}}=dz^{[L]} = \hat y -y$ 

#### 使用TensorFlow

![17](F:\program\neural_network\note_pictures\17.png)

![18](F:\program\neural_network\note_pictures\18.png)

![19](F:\program\neural_network\note_pictures\19.png)