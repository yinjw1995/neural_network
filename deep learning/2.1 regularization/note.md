- **基础**
  - 使用偏差和误差来优化神经网络
    - 先看偏差，如果偏差太高，就需要重新定义网络或者增加训练集，直到可以很好的拟合训练集为止（一般扩大网络可以很好的拟合）
    - 再看方差，如果方差太高，最好的解决办法是采用更多的数据，有时候办不到，则可以通过正则化来进行拟合，不断的尝试
    - 不停的尝试，找的两者都低的网络
  - 两者的权衡很重要，当数据足够多，并且，网络足够打是，可以做到保持一个不变而减小另一项，这就是下面要讲的正则化（减小方差的重要方法，一般不会是偏差增加特别大，特别是当网络足够大的时候。）

- **正则化**
  - 如果担心过拟合了而导致方差过大，那么最先想到的方法一般是正则化。或者是准备更多的数据，这是特别好的方法，但是一般难做到
  - L2是最常用的正则化，有人会觉得使用L1会降低存储，但实际上并不会改善太多而会使得模型变得稀疏
  - 正则化的缺点：计算量变大

- **为什么正则化可以防止过拟合**
  - cost function中包含参数w,b以及正则项L2范数，直观的理解是当正则化参数lambda被设置为很大的时候，权重矩阵w接近o，也就是说很多隐藏单元的权重为0（相当于影响特别小了），可以看做网络被简化了，类似逻辑回归单元但是深度很大，这是高度非线性的过拟合状态更接近线性，所以总会有一个中间值能做到恰好合适
  - 另一种直观的理解为：假设激活函数为tanh，当lambda很大，w就很小，z=wa+b,z因此很好，g(z)基本在线性区域，因此，整个网络的线性特征变强，无法进行高度非线性的表示，整个网络的计算值和线性函数的值不会相差太多，因此不会发生过拟合。
  - 在梯度下降调试时，其中一步是把cost function设计成单调下降的函数，但是在增加正则项之后，J相当于一个全新的函数，因此，需要对原来的J重新调整一下，否则可能会出现不单调的情况。

- **dropout正则化（随机失活）**
  - 将原始的网络复制一遍，dropout会历遍（go through）每一层，并设置相除网络中节点的概率，然后，精简网络，再用backprop进行训练。可以看出，网络实际上变小了。

  - 最常用的方法是inverted dropout（反向随机失活），以一个三层网络为例

    - $keep-prob=0.8$ 这表示每一个单元消失的概率为0.2

      $d3=np.random.randn(a3.shape[0],a3.shape[1])<keep-prob$

      $a3=np.multiply(a3,d3)$ d3为布尔数

      $a3/=keep-prob$ 因为a3减少了20%的数，为了不影响后面z的计算期望值，所以除以0.8，确保a3期望值不变

    - 在训练阶段，不同的训练样本会清除不同的隐藏单元，如果通过相同训练集多次传递数据，每次训练数据的梯度不同，则会对不同的单元进行归零，有时并不是这样，你需要对相同的单元进行归零。

    - 在测试阶段不需要进行dropout，因为，人们一般不希望预测的数据是随机的，同时，为了使结果准确还需要在额外的除以keep-prob，并且效果都差不多，所以没必要

- **理解dropout正则化**

  - dropout相当于使用了一个较小的网络，来达到正则化的效果
  - 另一种直观的理解就是，每个节点前面的特征可能被随机清除，因此，在训练过程中不会对某个特征给太多的权重，因此，该单元通过这种方式传开，为单元的每个输入增加一点权重，通过传播所有的权重，dropout将产生收缩权重平方范数的效果
  - dropout功能类似于L2正则化（因此也被看作为正则化的一种），只是应用的方式不同
  - 对于dropout的应用，每一层的keep-prob的设置根据每一层隐藏节点的数量而定，参数较多（也就是节点多）的层，设置小一点，类似于处理L2正则化参数lambda
  - 在计算机视觉中，通常会用到正则化，因为像素太多，并且没有足够的数据，但是在其他领域不一定， 一般数据足够就没必要使用。正则化的一个缺点就是cost function不再是明确的，从而失去了调试工具。

- **其他正则化方法**

  - 扩增数据，对于图片，可以通过随意翻转，剪切，扭曲等，通过变型处理，只要图片依然还可以辨认，基本是有效的，这种方法比正则化廉价
  - early stopping：一般，cost function是单调下降的，我们在验证集上面也是用同样的函数J，通过画出验证集上的成本函数可以看到，当迭代到某一步的时候，测试集的误差最小，我们就希望迭代过程就在此时停止，因此交early stopping。也可以理解为，初始的参数w很小，在迭代过程中，w慢慢变大，当w变化到中等大小的F范数时，与L2正则化一样选择相对不大的w
  - early stopping步骤：选择一个算法来优化代价函数J，可以使梯度下降或者是其他，我们不想过拟合，因此，在代价函数变小之后，通过另一个工具来控制参数w防止方差过大，一般就是正交化。这个方法的缺点就是不将两者分开考虑，只能同时考虑，所以需要考虑的参数就变多了。但是它的优点就是，只需要进行一次梯度下降就可以知道W的小，中间，最大值，而L2正则化则需要不停的搜索很多正则化参数lambda值，计算量就变大了

- **归一化输入（normalizing）**

  - 归一化可以加速训练网络
  - 第一步，每个数据减去均值$x=x-\mu$，第二步除以方差$x/=\sigma^2$,所以，每个数据的方程都是相同的，如果对训练集归一化，对测试集也要归一化，为了两个集都通过相同的转换可以放在一块转换。
  - 为什么要归一化：如果特征值x1,x2不在一个范围里，如x1=0-1000,x2=0-1，w1和w2就会相差较大，则代价函数在三维图上看上去就像一个狭长的碗，而归一化后就会相对均匀对称。如果不归一化，就需要使用较小的学习速率，而且路径比较曲折，迭代次数也较多，单如果函数在图上看上去像一个圆球形，使用梯度下降时，不管从哪开始，都可以直接的找到最小值，学习步长可以大一点。对于多维原理是一样的
  - 如果输入特征相差很大，0-1,0-1000，归一化很重要，要是相差不大，影响不大

- **梯度消失，梯度爆炸**

  - 尤其是在训练深层网络时，会出现导数非常大或小
  - 如果是很深的L层网络，假设最简单的$z=wx$， 当到L层是=时，$z^L=w^{L-1}x$ ,如果初始的w小于1，则$\hat{y}$基本为0，如果w大于1，则$\hat{h}$很大很大,对于激活函数来说，它是指数级的增加或减小，同样对于导数来说也是这样，因此，梯度下降需要使用很小的步长
  - 虽然不能彻底解决，但是可以在选择初始化权重的问题上可以有很大的帮助

  ​


